{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring euclidean distances between facial points caputred by nuiCapture as candidate features.\n",
    "<p>\n",
    "    <img src=\"nuicapture.png\">\n",
    "    <em>Source: CadavidConcepts</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as gl\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from typing import List\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelname(file_name):\n",
    "    label = file_name.replace(\"data\\points\\sample\", \"\").lower()\n",
    "    return label.replace(\".mat\", \"\")\n",
    "\n",
    "\n",
    "class Signal:\n",
    "    def __init__(self, x, y, label):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.label = label\n",
    "\n",
    "def dst(sig, points):\n",
    "    frame_x = np.split(sig.x, 5, axis=1)\n",
    "    frame_y = np.split(sig.y, 5, axis=1)\n",
    "    \n",
    "    nframes, nrecs, idx = np.shape(frame_x)\n",
    "    eucdist = np.zeros((nframes, nrecs))\n",
    "    p = points[0]\n",
    "    q = points[1]\n",
    "    \n",
    "    for f in range(nframes):\n",
    "        for r in range(nrecs):\n",
    "            u = frame_x[f][r][p], frame_y[f][r][p]\n",
    "            v = frame_x[f][r][q], frame_y[f][r][q]\n",
    "            eucdist[f][r] = distance.euclidean(u, v)\n",
    "    \n",
    "    return eucdist.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gl.glob(\"data\\points\\*.mat\")  # type: list\n",
    "signals = []  # type: List[Signal]\n",
    "\n",
    "for f in files:\n",
    "    data = loadmat(f).get('pontosSinal')\n",
    "    signals.append(Signal(data[:, ::2], data[:, 1::2], labelname(f)))\n",
    "\n",
    "n_signs = len(signals)\n",
    "n_recs, n_x = np.shape(signals[0].x)  # Number of recordings and number of features\n",
    "n_frames = 5\n",
    "\n",
    "# arbitrarily defined points, refer to notebook 'Distances'\n",
    "points = [[6, 3], [6, 11], [65, 32], [8, 9], [49, 16], [50, 17], [91, 92], [20, 25], [53, 58]]\n",
    "n_points, n_dim = np.shape(points)\n",
    "\n",
    "signals_feat = []  # Updated signals, according to each experiment\n",
    "signals_labels = []\n",
    "labels_dict = {}  # Dictionary of signals' labels, for reference\n",
    "i = 0\n",
    "\n",
    "for s in signals:\n",
    "    distances = []\n",
    "    for p in points:\n",
    "        distances.append(dst(s, p))     \n",
    "    signals_feat.append(np.hstack(distances))\n",
    "    signals_labels.append([i] * n_recs)\n",
    "    labels_dict[i] = s.label\n",
    "    i += 1\n",
    "\n",
    "sig_features = np.vstack(signals_feat)\n",
    "sig_labels = np.reshape(signals_labels, (n_signs * n_recs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration::  0\n",
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 16.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 25.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 31.4min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8442 tasks      | elapsed: 44.0min\n",
      "[Parallel(n_jobs=-1)]: Done 9792 tasks      | elapsed: 51.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11242 tasks      | elapsed: 59.0min\n",
      "[Parallel(n_jobs=-1)]: Done 12792 tasks      | elapsed: 67.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12960 out of 12960 | elapsed: 68.1min finished\n",
      "C:\\Users\\rubiarg\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7160e6b6d6de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mclass_report\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     feature_importance.append(pd.DataFrame(data=sorted(zip(map(lambda x: round(x, 4), \n\u001b[0m\u001b[0;32m     33\u001b[0m                                                                rf_grid.best_estimator_.feature_importances_), \n\u001b[0;32m     34\u001b[0m                                                            col_names), reverse=True)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "niter = 30\n",
    "results = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_report =[]\n",
    "class_report = []\n",
    "selected_params = []\n",
    "cm = [] # confusion matrix\n",
    "feature_importance = []\n",
    "col_names = range(1,1211)\n",
    "\n",
    "for i in range(niter):\n",
    "    print(\"Iteration:: \", i)\n",
    "    sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "    sss.get_n_splits(sig_features, sig_labels)\n",
    "\n",
    "    for train_index, test_index in sss.split(sig_features, sig_labels):\n",
    "        train_x, test_x = sig_features[train_index], sig_features[test_index]\n",
    "        train_y, test_y = sig_labels[train_index], sig_labels[test_index]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(estimator=rf, param_grid=random_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "    rf_grid.fit(train_x, train_y)\n",
    "    predictions = rf_grid.predict(test_x)\n",
    "    \n",
    "    selected_params.append(rf_grid.best_params_)\n",
    "    train_acc.append(accuracy_score(train_y, rf_grid.predict(train_x)))\n",
    "    test_acc.append(accuracy_score(test_y, predictions))\n",
    "    cm.append(confusion_matrix(test_y, predictions, labels=test_y))\n",
    "    class_report.append(classification_report(test_y, predictions, target_names=list(labels_dict.values())))\n",
    "    feature_importance.append(pd.DataFrame(data=sorted(zip(map(lambda x: round(x, 4), \n",
    "                                                               rf_grid.best_estimator_.feature_importances_), \n",
    "                                                           col_names), reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
