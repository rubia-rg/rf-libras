{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring euclidean distances between facial points caputred by nuiCapture as candidate features.\n",
    "<p>\n",
    "    <img src=\"nuicapture.png\">\n",
    "    <em>Source: CadavidConcepts</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob as gl\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from typing import List\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labelname(file_name):\n",
    "    label = file_name.replace(\"data/points/sample\", \"\").lower()\n",
    "    return label.replace(\".mat\", \"\")\n",
    "\n",
    "\n",
    "class Signal:\n",
    "    def __init__(self, x, y, label):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.label = label\n",
    "\n",
    "def dst(sig, points):\n",
    "    frame_x = np.split(sig.x, 5, axis=1)\n",
    "    frame_y = np.split(sig.y, 5, axis=1)\n",
    "    \n",
    "    nframes, nrecs, idx = np.shape(frame_x)\n",
    "    eucdist = np.zeros((nframes, nrecs))\n",
    "    p = points[0]\n",
    "    q = points[1]\n",
    "    \n",
    "    for f in range(nframes):\n",
    "        for r in range(nrecs):\n",
    "            u = frame_x[f][r][p], frame_y[f][r][p]\n",
    "            v = frame_x[f][r][q], frame_y[f][r][q]\n",
    "            eucdist[f][r] = distance.euclidean(u, v)\n",
    "    \n",
    "    return eucdist.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = gl.glob(\"data/points/*.mat\")  # type: list\n",
    "signals = []  # type: List[Signal]\n",
    "\n",
    "for f in files:\n",
    "    data = loadmat(f).get('pontosSinal')\n",
    "    signals.append(Signal(data[:, ::2], data[:, 1::2], labelname(f)))\n",
    "\n",
    "n_signs = len(signals)\n",
    "n_recs, n_x = np.shape(signals[0].x)  # Number of recordings and number of features\n",
    "n_frames = 5\n",
    "\n",
    "# arbitrarily defined points, refer to notebook 'Distances'\n",
    "points = [[6, 3], [6, 11], [65, 32], [8, 9], [49, 16], [50, 17], [91, 92], [20, 25], [53, 58]]\n",
    "n_points, n_dim = np.shape(points)\n",
    "\n",
    "signals_feat = []  # Updated signals, according to each experiment\n",
    "signals_labels = []\n",
    "labels_dict = {}  # Dictionary of signals' labels, for reference\n",
    "i = 0\n",
    "\n",
    "for s in signals:\n",
    "    distances = []\n",
    "    for p in points:\n",
    "        distances.append(dst(s, p))     \n",
    "    signals_feat.append(np.hstack(distances))\n",
    "    signals_labels.append([i] * n_recs)\n",
    "    labels_dict[i] = s.label\n",
    "    i += 1\n",
    "\n",
    "sig_features = np.vstack(signals_feat)\n",
    "sig_labels = np.reshape(signals_labels, (n_signs * n_recs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 45)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "niter = 30\n",
    "results = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_report =[]\n",
    "class_report = []\n",
    "selected_params = []\n",
    "cm = [] # confusion matrix\n",
    "feature_importance = []\n",
    "col_names = range(1,1211)\n",
    "\n",
    "for i in range(niter):\n",
    "    print(\"Iteration:: \", i)\n",
    "    sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "    sss.get_n_splits(sig_features, sig_labels)\n",
    "\n",
    "    for train_index, test_index in sss.split(sig_features, sig_labels):\n",
    "        train_x, test_x = sig_features[train_index], sig_features[test_index]\n",
    "        train_y, test_y = sig_labels[train_index], sig_labels[test_index]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(estimator=rf, param_grid=random_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "    rf_grid.fit(train_x, train_y)\n",
    "    predictions = rf_grid.predict(test_x)\n",
    "    \n",
    "    selected_params.append(rf_grid.best_params_)\n",
    "    train_acc.append(accuracy_score(train_y, rf_grid.predict(train_x)))\n",
    "    test_acc.append(accuracy_score(test_y, predictions))\n",
    "    cm.append(confusion_matrix(test_y, predictions, labels=test_y))\n",
    "    class_report.append(classification_report(test_y, predictions, target_names=list(labels_dict.values())))\n",
    "    feature_importance.append(pd.DataFrame(data=sorted(zip(map(lambda x: round(x, 4), \n",
    "                                                               rf_grid.best_estimator_.feature_importances_), \n",
    "                                                           col_names), reverse=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
