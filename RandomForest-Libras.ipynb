{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest + LIBRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers a Python-based solution to be compared to the experiments 1 to 4 presented in <cite data-cite=\"6013574/XD5B9TZQ\"></cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presented solution consists in reproducing the feature vectors for each of the experiments utilizing (x, y) points in <cite data-cite=\"6013574/XD5B9TZQ\"></cite>, applying a RandomForest classifier, and lastly, comparing the resulting performance to the ones obtained by SVM and k-NN approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as gl\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelname(file_name):  # Extract labels from filenames \n",
    "    label = file_name.replace(\"data\\points\\sample\", \"\").lower()\n",
    "    return label.replace(\".mat\", \"\")\n",
    "\n",
    "\n",
    "class Signal:  # Signal representation containing x and y coordinates and corresponding label\n",
    "    def __init__(self, x, y, label):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment utilizes each signal in its raw form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigvalues(sig):  # EX.1: raw data\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    updsig[:, ::2] = sig.x\n",
    "    updsig[:, 1::2] = sig.y\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second experiment consists of applying z-normalization to each sample. The updated coordinates are:\n",
    "\\begin{align}\n",
    "x_{\\mathcal{N(0,1)}}=\\frac{x-\\bar{x}}{\\sigma(x)} \\\\\n",
    "y_{\\mathcal{N(0,1)}}=\\frac{y-\\bar{y}}{\\sigma(y)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigvalues(sig):  # EX.2: z-norm\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    for idx, x in enumerate(sig.x):\n",
    "        sig.x[idx] = np.divide((np.transpose(x) - np.mean(x)), np.std(x))\n",
    "    for idx, y in enumerate(sig.y):\n",
    "        sig.y[idx] = np.divide((np.transpose(y) - np.mean(y)), np.std(y))\n",
    "\n",
    "    updsig[:, ::2] = sig.x\n",
    "    updsig[:, 1::2] = sig.y\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third experiment consists of normalizing each signal by its 1st frame centroid, as follows:\n",
    "\\begin{align}\n",
    "\\tilde{x}_{P,f}=x_{P,f}-\\bar{x}_{1} \\\\\n",
    "\\tilde{y}_{P,f}=y_{P,f}-\\bar{y}_{1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigvalues(sig): # EX.3: norm by 1st frame centroid\n",
    "    frame_x = np.split(sig.x, 5, axis=1)  # x-coordinates by frame\n",
    "    frame_y = np.split(sig.y, 5, axis=1)  # y-coordinates by frame\n",
    "    cent_x, cent_y = (np.mean(frame_x[0], axis=1), np.mean(frame_y[0], axis=1))  # first frame centroid of each recording\n",
    "\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    for idx, x in enumerate(sig.x):\n",
    "        sig.x[idx] = x - cent_x[idx]\n",
    "    for idx, y in enumerate(sig.y):\n",
    "        sig.y[idx] = y - cent_y[idx]\n",
    "\n",
    "    updsig[:, ::2] = sig.x\n",
    "    updsig[:, 1::2] = sig.y\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth experiment consists of normalizing each signal by its current frame centroid, as follows:\n",
    "\\begin{align}\n",
    "\\tilde{x}_{P,f}=x_{P,f}-\\bar{x}_{f} \\\\\n",
    "\\tilde{y}_{P,f}=y_{P,f}-\\bar{y}_{f}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigvalues(sig):  # EX.4\n",
    "    frame_x = np.split(sig.x, 5, axis=1)  # x-coordinates by frame\n",
    "    frame_y = np.split(sig.y, 5, axis=1)  # y-coordinates by frame\n",
    "    cent_x, cent_y = (np.mean(frame_x, axis=2), np.mean(frame_y, axis=2))  # centroids of each recording\n",
    "\n",
    "    nframes, nrecs, idx = np.shape(frame_x)\n",
    "    updsig = np.zeros((np.shape(sig.x)[0], np.shape(sig.x)[1] * 2))\n",
    "    for fx in range(nframes):\n",
    "        frame_x[fx] = np.transpose(np.transpose(frame_x[fx]) - cent_x[fx])\n",
    "    for fy in range(nframes):\n",
    "        frame_y[fy] = np.transpose(np.transpose(frame_y[fy]) - cent_y[fy])\n",
    "\n",
    "    updsig[:, ::2] = np.hstack(frame_x)\n",
    "    updsig[:, 1::2] = np.hstack(frame_y)\n",
    "\n",
    "    return updsig  # Updated signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For means of comparison, this solution utilizes the Random Forest algorithm for the classification task. Tuning is done on every iteration utilizing GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gl.glob(\"data\\points\\*.mat\")  # type: list\n",
    "\n",
    "signals = []  # type: List[Signal]\n",
    "\n",
    "for f in files:\n",
    "    data = loadmat(f).get('pontosSinal')\n",
    "    signals.append(Signal(data[:, ::2], data[:, 1::2], labelname(f)))\n",
    "\n",
    "n_signs = len(signals)\n",
    "n_recs, n_x = np.shape(signals[0].x)  # Number of recordings and number of features\n",
    "\n",
    "signals_feat = []  # Updated signals, according to each experiment\n",
    "signals_labels = []\n",
    "labels_dict = {}  # Dictionary of signals' labels, for reference\n",
    "i = 0\n",
    "\n",
    "for s in signals:\n",
    "    signals_feat.append(sigvalues(s))\n",
    "    signals_labels.append([i] * n_recs)\n",
    "    labels_dict[i] = s.label\n",
    "    i += 1\n",
    "    \n",
    "sig_features = np.reshape(signals_feat, (n_signs * n_recs, n_x * 2))\n",
    "sig_labels = np.reshape(signals_labels, (n_signs * n_recs,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO: PARAMETER SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['log2', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration::  0\n",
      "Fitting 3 folds for each of 4320 candidates, totalling 12960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 24.5min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 30.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 36.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7192 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8442 tasks      | elapsed: 52.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9792 tasks      | elapsed: 61.5min\n",
      "[Parallel(n_jobs=-1)]: Done 11242 tasks      | elapsed: 71.1min\n",
      "[Parallel(n_jobs=-1)]: Done 12792 tasks      | elapsed: 81.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12960 out of 12960 | elapsed: 82.5min finished\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a0ba7a5ed0e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mselected_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "niter = 30\n",
    "results = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_report =[]\n",
    "class_report = []\n",
    "selected_params = []\n",
    "cm = [] # confusion matrix\n",
    "feature_importance = []\n",
    "col_names = range(1,1211)\n",
    "\n",
    "for i in range(niter):\n",
    "    print(\"Iteration:: \", i)\n",
    "    sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=42)\n",
    "    sss.get_n_splits(sig_features, sig_labels)\n",
    "\n",
    "    for train_index, test_index in sss.split(sig_features, sig_labels):\n",
    "        train_x, test_x = sig_features[train_index], sig_features[test_index]\n",
    "        train_y, test_y = sig_labels[train_index], sig_labels[test_index]\n",
    "\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(estimator=rf, param_grid=random_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "    rf_grid.fit(train_x, train_y)\n",
    "    predictions = rf_grid.predict(test_x)\n",
    "    \n",
    "    selected_params.append(rf_grid.best_params_)\n",
    "    train_acc.append(accuracy_score(train_y, rf_grid.predict(train_x)))\n",
    "    test_acc.append(accuracy_score(test_y, predictions))\n",
    "    cm.append(confusion_matrix(test_y, predictions, labels=test_y))\n",
    "    class_report.append(classification_report(test_y, predictions, target_names=list(labels_dict.values())))\n",
    "    feature_importance.append(pd.DataFrame(data=sorted(zip(map(lambda x: round(x, 4), \n",
    "                                                               rf_grid.best_estimator_.feature_importances_), \n",
    "                                                           col_names), reverse=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['k-NN', 'SVM', 'RandomForest']\n",
    "col_names = ['EX.1-mean', 'EX.1-std', 'EX.2-mean', 'EX.2-std', 'EX.3-mean', 'EX.3-std', 'EX.4-mean', 'EX.4-std']\n",
    "results = [['75.50%', '8.44', '74.50%', '6.06', '67.66%', '6.78', '77.83%', '6.52'], \n",
    "           ['82.50%', '6.79', '76.83%', '7.59', '76.50%', '8.11', '80.66%', '7.15'], \n",
    "           ['68.33%', '2.73', '72.00%', '3.11', '70.17%', '1.60', '71.17%', '2.52']]\n",
    "results_all = pd.DataFrame(data=results,index=class_names,columns=col_names)\n",
    "results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"cite2c-biblio\"></div>"
   ]
  }
 ],
 "metadata": {
  "cite2c": {
   "citations": {
    "6013574/QDJUJ23A": {
     "URL": "http://urlib.net/8JMKD3MGPAW/3MDH39S",
     "author": [
      {
       "family": "Rezende",
       "given": "Tamires"
      },
      {
       "family": "Castro",
       "given": "Cristiano"
      },
      {
       "family": "Almeida",
       "given": "Sílvia"
      }
     ],
     "container-title": "CONFERENCE ON GRAPHICS, PATTERNS AND IMAGES (SIBGRAPI)",
     "issued": {
      "date-parts": [
       [
        2016
       ]
      ]
     },
     "title": "An approach for Brazilian Sign Language (BSL) recognition based on facial expression and k-NN classifier",
     "type": "article-journal",
     "volume": "29"
    },
    "6013574/XD5B9TZQ": {
     "URL": "https://www.ppgee.ufmg.br/defesas/1393M.PDF",
     "author": [
      {
       "family": "Rezende",
       "given": "Tamires"
      }
     ],
     "genre": "Master Thesis",
     "id": "6013574/XD5B9TZQ",
     "issued": {
      "year": 2016
     },
     "language": "Portuguese",
     "publisher": "UFMG",
     "title": "Aplicação de Técnicas de Inteligência Computacional para Análise da Expressão Facial em Reconhecimento de Sinais de Libras",
     "type": "thesis"
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
